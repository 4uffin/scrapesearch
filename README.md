# **ScrapeSearch**

## **Overview**

ScrapeSearch.py is a versatile Python command-line tool designed for scraping search results from various search engines and extracting content from the resulting web pages. It is built to be flexible and efficient, supporting both single queries and batch processing from a file, with options for customizable output.

## **Features**

* **Multi-Engine Support:** Scrape results from **Google**, **Bing**, and **DuckDuckGo**.  
* **Two Modes of Operation:** Use it from the command line for automation or in an interactive mode for on-the-fly queries.  
* **Batch Processing:** Scrape multiple queries by providing a list of search terms in a text file.  
* **Parallel Processing:** Speeds up the scraping process by fetching multiple pages concurrently using a thread pool.  
* **Configurable Settings:** Easily customize scraping behavior such as the number of URLs to scrape, timeouts, and output fields using a config.json file.  
* **Flexible Output:** Save scraped data in either **JSON** or **CSV** format.  
* **Verbose Logging:** An optional verbose mode provides real-time feedback on the scraping progress.

## **Prerequisites**

To run this script, you need to have Python 3 installed. You'll also need to install the following Python libraries:

pip install requests beautifulsoup4 tqdm

* **requests**: For making HTTP requests to fetch web pages.  
* **beautifulsoup4**: For parsing HTML and extracting data from web pages.  
* **tqdm**: For displaying a progress bar during the scraping process.

## **Configuration**

The script's behavior can be customized by editing the config.json file. If this file is not found, the script will use the default values shown below.

{  
    "num\_urls": 5,  
    "timeout": 10,  
    "max\_workers": 5,  
    "retries": 3,  
    "output\_format": "json",  
    "output\_fields": \[  
        "url",  
        "title",  
        "description",  
        "full\_content"  
    \],  
    "search\_engine": "google"  
}

* num\_urls: The number of search results to scrape for each query.  
* timeout: The maximum number of seconds to wait for a server response before retrying.  
* max\_workers: The number of concurrent threads to use for scraping pages.  
* retries: The number of times to retry a failed HTTP request.  
* output\_format: The default file format for saving results (json or csv).  
* output\_fields: A list of data fields to include in the output file.  
* search\_engine: The default search engine to use (google, bing, or duckduckgo).

You can override these settings via command-line arguments.

## **Command-Line Usage**

The script is executed from your terminal. Use the \-h or \--help flag to see all available options.

python scrapesearch.py \--help

### **Examples**

**1\. Basic search for a single query:**

This command performs a search for "best python web scrapers" and scrapes the first 5 results (based on config.json), saving the output to scraped\_results.json by default.

python scrapesearch.py "best python web scrapers"

**2\. Scrape from an input file and save as CSV:**

This command reads queries from a file named queries.txt, scrapes 3 URLs for each query, and saves all the results to a single file named batch\_results.csv.

python scrapesearch.py \-i queries.txt \-n 3 \-f csv \-o batch\_results.csv

**3\. Search only (no scraping):**

This command will perform the search but will not scrape the pages. It simply prints the titles and URLs of the results to the console.

python scrapesearch.py "famous landmarks" \--search-only

## **Interactive Usage**

To enter the interactive mode, simply run the script without any arguments.

python scrapesearch.py

The script will prompt you to enter a search query, scrape the results, and then ask if you want to save the data. This is useful for quick, ad-hoc searches without needing to remember all the command-line flags.

## **Output**

The scraped data can be saved as either a **JSON** or **CSV** file.

### **JSON Output**

The output is a list of dictionaries, where each dictionary represents a scraped page and contains the fields you specified in the configuration (e.g., url, title, description, full\_content).

\[  
    {  
        "url": "https://example.com/article1",  
        "title": "Example Article",  
        "description": "A brief summary of the article.",  
        "full\_content": "The full text content of the article..."  
    },  
    ...  
\]

### **CSV Output**

The output is a comma-separated values file where the first row contains the field names and subsequent rows contain the corresponding data.

---

> Generated by Google Gemini
